{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 第二次课\n",
    "1.前30分钟，检测大家的python水平（最好控制在30分钟左右）。\n",
    "\n",
    "2.后30分钟，讲jieba/HanLP/nltk三个分词工具。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 测验&作业"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 测验：https://www.runoob.com/quiz/python-quiz.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数据类型\n",
    "# 题目：生成一个5位数，要求：一、求它是几位数，二、逆序打印出各位数字。\n",
    "import random\n",
    "\n",
    "# 最后输出要求：[5,25341]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# 字符串\n",
    "# 题目：反转字符串，输入\"hello\"，输出\"olleh\"\n",
    "string = \"hello\"\n",
    "\n",
    "# 最后要求输出：\"olleh\"，使用三种方法。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 循环\n",
    "# 题目：有四个数字：1、2、3、4，能组成多少个互不相同且无重复数字的三位数？各是多少？\n",
    "i,j,k,l = 1,2,3,4\n",
    "\n",
    "# 最后要求输出：[123,124,234,134,......]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 列表\n",
    "# 题目：给定按相反的顺序输出列表的值，并且去重。\n",
    "li = [4, 3, 2, 1, 1, 2, 3, 6, 2, 2, 3, 6, 2, 3, 7, 3, 3]\n",
    "\n",
    "# 最后要求输出：相反的list,且不能有重复。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 函数\n",
    "# 给定一个name_list = ['adam', 'LISA', 'barT'],输出首字母大写，后续字母小写的list\n",
    "def normalize(name):\n",
    "    pass\n",
    "\n",
    "# 最后要求输出name_list = ['Adam', 'List', 'bart']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lambda函数\n",
    "# 题目：请将列表[1,2,3,4,5]使用python方法转变成[1,4,9,16,25]。然后提取大于10的数，最终输出[16,25]\n",
    "li = [1,2,3,4,5]\n",
    "\n",
    "# 最后要求输出:[16,25]，必须使用lambda方法。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 算法使用部分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import jieba\n",
    "import pandas as pd\n",
    "from pyhanlp import *\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = []\n",
    "with open(\"stopwords.txt\",\"r\",encoding = \"utf-8\") as f:\n",
    "    for line in f.readlines():\n",
    "        stopwords.append(line.strip(\"\\n\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "red_chamber_table = pd.read_table(\"红楼梦.txt\",sep='\\t',header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 打开停用词词表\n",
    "stopwords = []\n",
    "with open(\"stopwords.txt\",\"r\",encoding = \"utf-8\") as f:\n",
    "    for line in f.readlines():\n",
    "        stopwords.append(line.strip(\"\\n\"))\n",
    "        \n",
    "# 载入红楼梦全本，或上面方法打开\n",
    "red_chamber_table = pd.read_table(\"红楼梦.txt\",sep='\\t',header=None)\n",
    "corpus = \"\".join(red_chamber_table[0])\n",
    "\n",
    "# 精准模式切分\n",
    "word_list = jieba.lcut(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(word_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 清洗\n",
    "word_list_clean = []\n",
    "for word in word_list:\n",
    "    if word not in stopwords:\n",
    "        word_list_clean.append(word)\n",
    "    \n",
    "# 统计词频\n",
    "freq_table = pd.DataFrame([dict(Counter(word_list_clean))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_table = freq_table.T\n",
    "freq_table.columns = [\"freq\"]\n",
    "freq_table.sort_values(\"freq\",ascending = False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jieba.add_word(\"贾宝玉\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# jieba除了精准模式（即默认），还有全切分模式和搜索切分模式\n",
    "# 全切分：速度非常快，但不能解决歧义。\n",
    "segs_all = jieba.cut(corpus, cut_all=True)\n",
    "result = \" \".join(segs_all)\n",
    "\n",
    "# 可以写成lcut\n",
    "# 搜索引擎模式：在精确模式的基础上，对长词再次切分，提高召回率，适合用于搜索引擎分词\n",
    "segs_4 = jieba.cut_for_search(corpus)\n",
    "result = \" \".join(segs_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 获取词性\n",
    "import jieba.posseg as psg\n",
    "segs_5 = [(x.word,x.flag) for x in psg.lcut(corpus)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "segs_5[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 思考问题：\n",
    "\"\"\"\n",
    "# 我们能不能按照章回，将所有文字切分成一个表格？\n",
    "# 然后来分析每一回的用词，词频？\n",
    "# 使用的最多的动词？使用的最多的名词？使用的最多的形容词？\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 下节课：正则表达式讲解"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 算法原理部分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import jieba\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####\n",
    "# 以词典的为基础的算法，脱不开全切分、正向匹配/反向匹配/双向匹配四种。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 全切分算法\n",
    "from pyhanlp import *\n",
    "\n",
    "def load_dictionary():\n",
    "    IOUtil = JClass(\"com.hankcs.hanlp.corpus.io.IOUtil\")\n",
    "    path = HanLP.Config.CoreDictionaryPath.replace(\".txt\",\".mini.txt\")\n",
    "    dic = IOUtil.loadDictionary([path])\n",
    "    return set(dic.keySet())\n",
    "\n",
    "def fully_segment(text,dic):\n",
    "    word_list = []\n",
    "    for i in range(len(text)):\n",
    "        for j in range(i + 1,len(text) + 1):\n",
    "            word = text[i:j]\n",
    "            if word in dic:\n",
    "                word_list.append(word)\n",
    "    return word_list\n",
    "\n",
    "# 执行\n",
    "dic = load_dictionary()\n",
    "fully_segment(\"商品和服务\",dic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 正向最长匹配\n",
    "def forward_segment(text,dic):\n",
    "    word_list = []\n",
    "    i = 0 \n",
    "    while i < len(text):\n",
    "        longest_word = text[i]\n",
    "        for j in range(i+1,len(text) + 1):\n",
    "            word = text[i:j]\n",
    "            if word in dic:\n",
    "                if len(word) > len(longest_word):\n",
    "                    longest_word = word\n",
    "        word_list.append(longest_word)\n",
    "        i += len(longest_word)\n",
    "    return word_list\n",
    "\n",
    "string = \"研究生命起源\"\n",
    "forward_segment(string,dic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 反向最长匹配\n",
    "def backward_segment(text,dic):\n",
    "    word_list = []\n",
    "    i = len(text) - 1\n",
    "    while i >= 0:\n",
    "        longest_word = text[i]\n",
    "        for j in range(0,i):\n",
    "            word = text[j:i + 1]\n",
    "            if word in dic:\n",
    "                if len(word) > len(longest_word):\n",
    "                    longest_word = word\n",
    "        print(word_list)\n",
    "        word_list.insert(0,longest_word)\n",
    "        i -= len(longest_word)\n",
    "    return word_list\n",
    "\n",
    "backward_segment(string,dic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 双向最长匹配\n",
    "def count_single_char(word_list:list):\n",
    "    return sum(1 for word in word_list if len(word) == 1)\n",
    "\n",
    "def bidirectional_segment(text,dic):\n",
    "    f = forward_segment(text,dic)\n",
    "    b = backward_segment(text,dic)\n",
    "    if len(f) < len(b):\n",
    "        return f\n",
    "    elif len(f) > len(b):\n",
    "        return b\n",
    "    else:\n",
    "        if count_single_char(f) < count_single_char(b):\n",
    "            return f\n",
    "        else:\n",
    "            return b\n",
    "        \n",
    "bidirectional_segment(string,dic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_speed(segment,text,dic):\n",
    "    start_time = time.time()\n",
    "    for i in range(pressure):\n",
    "        segment(text,dic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 速度测试\n",
    "def evaluate_speed(segment, text, dic):\n",
    "    start_time = time.time()\n",
    "    for i in range(pressure):\n",
    "        segment(text, dic)\n",
    "    elapsed_time = time.time() - start_time\n",
    "    print('%.2f 万字/秒' % (len(text) * pressure / 10000 / elapsed_time))\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    text = \"江西鄱阳湖干枯，中国最大淡水湖变成大草原\"\n",
    "    pressure = 10000\n",
    "    dic = load_dictionary()\n",
    "\n",
    "    print('由于JPype调用开销巨大，以下速度显著慢于原生Java')\n",
    "    evaluate_speed(forward_segment, text, dic)\n",
    "    evaluate_speed(backward_segment, text, dic)\n",
    "    evaluate_speed(bidirectional_segment, text, dic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 想要加速这个词典模型？散列表太慢了，必须用自己构造的字典树。\n",
    "# 构建一颗字典树：\n",
    "class Node(object):\n",
    "    def __init__(self, value) -> None:\n",
    "        self._children = {}\n",
    "        self._value = value\n",
    "\n",
    "    def _add_child(self, char, value, overwrite=False):\n",
    "        child = self._children.get(char)\n",
    "        if child is None:\n",
    "            child = Node(value)\n",
    "            self._children[char] = child\n",
    "        elif overwrite:\n",
    "            child._value = value\n",
    "        return child\n",
    "\n",
    "\n",
    "class Trie(Node):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__(None)\n",
    "\n",
    "    def __contains__(self, key):\n",
    "        return self[key] is not None\n",
    "\n",
    "    def __getitem__(self, key):\n",
    "        state = self\n",
    "        for char in key:\n",
    "            state = state._children.get(char)\n",
    "            if state is None:\n",
    "                return None\n",
    "        return state._value\n",
    "\n",
    "    def __setitem__(self, key, value):\n",
    "        state = self\n",
    "        for i, char in enumerate(key):\n",
    "            if i < len(key) - 1:\n",
    "                state = state._add_child(char, None, False)\n",
    "            else:\n",
    "                state = state._add_child(char, value, True)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    trie = Trie()\n",
    "    # 增\n",
    "    hash()\n",
    "    trie['自然'] = 'nature'\n",
    "    trie['自然人'] = 'human'\n",
    "    trie['自然语言'] = 'language'\n",
    "    trie['自语'] = 'talk\tto oneself'\n",
    "    trie['入门'] = 'introduction'\n",
    "    assert '自然' in trie\n",
    "    # 删\n",
    "    trie['自然'] = None\n",
    "    assert '自然' not in trie\n",
    "    # 改\n",
    "    trie['自然语言'] = 'human language'\n",
    "    assert trie['自然语言'] == 'human language'\n",
    "    # 查\n",
    "    assert trie['入门'] == 'introduction'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 一颗AC字典树的应用\n",
    "from pyhanlp import *\n",
    "\n",
    "def classic_demo():\n",
    "    words = [\"hers\", \"his\", \"she\", \"he\"]\n",
    "    Trie = JClass('com.hankcs.hanlp.algorithm.ahocorasick.trie.Trie')\n",
    "    trie = Trie()\n",
    "    for w in words:\n",
    "        trie.addKeyword(w)\n",
    "\n",
    "    for emit in trie.parseText(\"ushers\"):\n",
    "        print(\"[%d:%d]=%s\" % (emit.getStart(), emit.getEnd(), emit.getKeyword()))\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    classic_demo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node(object):\n",
    "    def __init__(self,value) -> None:\n",
    "        self._children = {}\n",
    "        self._value = value\n",
    "        \n",
    "    def _add_child(self,char,value,overwrite = False):\n",
    "        child = self._children.get(char)\n",
    "        if child is None:\n",
    "            child = Node(value)\n",
    "            self._children[char] = child\n",
    "        elif overwrite:\n",
    "            child._value = value\n",
    "        return child\n",
    "    \n",
    "class Trie(Node):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__(None)\n",
    "    \n",
    "    def __contains__(self,key):\n",
    "        return self[key] is not None\n",
    "    \n",
    "    def __getitem__(self,key):\n",
    "        state = self\n",
    "        for char in key:\n",
    "            print(char)\n",
    "            state = state._children.get(char)\n",
    "            if state is None:\n",
    "                return None\n",
    "        return state._value\n",
    "    \n",
    "    def __setitem__(self,key,value):\n",
    "        state = self\n",
    "        for i,char in enumerate(key):\n",
    "            if i < len(key) - 1:\n",
    "                state = state._add_child(char,None,False)\n",
    "            else:\n",
    "                print(value)\n",
    "                state = state._add_child(char,value,True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BeiMenChuiXue:\n",
    "    def __init__(self,name):\n",
    "        self.name = name\n",
    "        \n",
    "    def __contains__(self,item):\n",
    "        return item is not None\n",
    "    \n",
    "name = BeiMenChuiXue(\"beimenchuixue\")\n",
    "print(None in name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义双数组字典树\n",
    "# dat\n",
    "from pyhanlp import *\n",
    "\n",
    "class DoubleArrayTrie(object):\n",
    "    def __init__(self,dic:dict) -> None:\n",
    "        m = JClass('java.util.TreeMap')()\n",
    "        for k,v in dic.items():\n",
    "            m[k] = v\n",
    "        dat = JClass(\"com.hankcs.hanlp.collection.trie.DoubleArrayTrie\")(m)\n",
    "        \n",
    "        DoubleArrayTrie = JClass('com.hankcs.hanlp.collection.trie.DoubleArrayTrie')\n",
    "        dat = DoubleArrayTrie(m)\n",
    "        # 自jpype 0.7起，使用反射技术获取私有成员，等效于下列三句话\n",
    "        # self.base = dat.base\n",
    "        # self.check = dat.check\n",
    "        # self.value = dat.v\n",
    "        DoubleArrayTrie.class_.getDeclaredField('base').setAccessible(True)\n",
    "        DoubleArrayTrie.class_.getDeclaredField('check').setAccessible(True)\n",
    "        DoubleArrayTrie.class_.getDeclaredField('v').setAccessible(True)\n",
    "        self.base = DoubleArrayTrie.class_.getDeclaredField('base').get(dat)\n",
    "        self.check = DoubleArrayTrie.class_.getDeclaredField('check').get(dat)\n",
    "        self.value = DoubleArrayTrie.class_.getDeclaredField('v').get(dat)\n",
    "\n",
    "        \n",
    "    def char_hash(self,c) -> int:\n",
    "        return JClass('java.lang.Character')(c).hashCode()\n",
    "\n",
    "    def transition(self,c,b) -> int:\n",
    "        \"\"\"\n",
    "        状态转移\n",
    "        :param c:字符\n",
    "        :param b:初始状态\n",
    "        :return 转移后的状态,-1表示失败\n",
    "        \"\"\"\n",
    "        p = self.base[b] + self.char_hash(c) + 1\n",
    "        if self.base[b] == self.check[p]:\n",
    "            return p\n",
    "        else:\n",
    "            return -1\n",
    "        \n",
    "    def __getitem__(self,key:str):\n",
    "        b = 0\n",
    "        for i in range(0,len(key)):\n",
    "            p = self.transition(key[i],b)\n",
    "            if p is not -1:\n",
    "                b = p\n",
    "            else:\n",
    "                return None\n",
    "            \n",
    "        p = self.base[b]\n",
    "        n = self.base[p]\n",
    "        if p == self.check[p] and n < 0:\n",
    "            index = -n - 1\n",
    "            return self.value[index]\n",
    "        return None\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    dic = {\"自然\":\"nature\",\"自然人\":\"human\",\"自然语言\":\"language\",\n",
    "          \"自语\":\"talk to oneself\",\"入门\":\"introduction\"}\n",
    "    \n",
    "    dat = DoubleArrayTrie(dic)\n",
    "    print(dat[\"自然人\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用AC自动机的全切分\n",
    "words = [\"hers\",\"his\",\"she\",\"he\"]\n",
    "Trie = JClass(\"com.hankcs.hanlp.algorithm.ahocorasick.trie.Trie\")\n",
    "trie = Trie()\n",
    "for w in words:\n",
    "    trie.addKeyword(w)\n",
    "    \n",
    "for emit in trie.parseText(\"ushers\"):\n",
    "    print(\"[%d:%d]=%s\" % (emit.getStart(),emit.getEnd(),emit.getKeyword()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HanLP中的封装\n",
    "from pyhanlp import *\n",
    "from pyhanlp.static import HANLP_DATA_PATH\n",
    "\n",
    "HanLP.Config.ShowTermNature = True\n",
    "segment = DoubleArrayTrieSegment()\n",
    "print(segment.seg('江西鄱阳湖干枯，中国最大淡水湖变成大草原'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HanLP中的封装:dat\n",
    "from pyhanlp import *\n",
    "from pyhanlp.static import HANLP_DATA_PATH\n",
    "\n",
    "HanLP.Config.ShowTermNature = False\n",
    "segment = DoubleArrayTrieSegment()\n",
    "print(segment.seg('江西鄱阳湖干枯，中国最大淡水湖变成大草原'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HanLP中的封装:ACDAT\n",
    "from pyhanlp import *\n",
    "\n",
    "HanLP.Config.ShowTermNature = False\n",
    "segment = JClass('com.hankcs.hanlp.seg.Other.AhoCorasickDoubleArrayTrieSegment')()\n",
    "print(segment.seg(\"江西鄱阳湖干枯，中国最大淡水湖变成大草原\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 精准率、召回率、F1值\n",
    "# 混淆矩阵"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "分词结果： [停用, 词, 的, 意义, 相对而言, 无关紧要, 吧, 。]\n",
      "分词结果去除停用词： ['停用', '词', '意义', '无关紧要']\n",
      "不分词去掉停用词 停用词**意义**无关紧要**。\n"
     ]
    }
   ],
   "source": [
    "# 停用词\n",
    "from jpype import JString\n",
    "from pyhanlp import *\n",
    "\n",
    "def load_from_file(path):\n",
    "    \"\"\"\n",
    "    从词典文件加载DoubleArrayTrie\n",
    "    :param path: 词典路径\n",
    "    :return: 双数组trie树\n",
    "    \"\"\"\n",
    "    map = JClass('java.util.TreeMap')()  # 创建TreeMap实例\n",
    "    with open(path) as src:\n",
    "        for word in src:\n",
    "            word = word.strip()  # 去掉Python读入的\\n\n",
    "            map[word] = word\n",
    "    return JClass('com.hankcs.hanlp.collection.trie.DoubleArrayTrie')(map)\n",
    "\n",
    "def load_from_words(*words):\n",
    "    \"\"\"\n",
    "    从词汇构造双数组trie树\n",
    "    :param words: 一系列词语\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    map = JClass('java.util.TreeMap')()  # 创建TreeMap实例\n",
    "    for word in words:\n",
    "        map[word] = word\n",
    "    return JClass('com.hankcs.hanlp.collection.trie.DoubleArrayTrie')(map)\n",
    "\n",
    "def remove_stopwords_termlist(termlist, trie):\n",
    "    return [term.word for term in termlist if not trie.containsKey(term.word)]\n",
    "\n",
    "def replace_stropwords_text(text, replacement, trie):\n",
    "    searcher = trie.getLongestSearcher(JString(text), 0)\n",
    "    offset = 0\n",
    "    result = ''\n",
    "    while searcher.next():\n",
    "        begin = searcher.begin\n",
    "        end = begin + searcher.length\n",
    "        if begin > offset:\n",
    "            result += text[offset: begin]\n",
    "        result += replacement\n",
    "        offset = end\n",
    "    if offset < len(text):\n",
    "        result += text[offset]\n",
    "    return result\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    HanLP.Config.ShowTermNature = False\n",
    "    trie = load_from_file(HanLP.Config.CoreStopWordDictionaryPath)\n",
    "    text = \"停用词的意义相对而言无关紧要吧。\"\n",
    "    segment = DoubleArrayTrieSegment()\n",
    "    termlist = segment.seg(text)\n",
    "    print(\"分词结果：\", termlist)\n",
    "    print(\"分词结果去除停用词：\", remove_stopwords_termlist(termlist, trie))\n",
    "    trie = load_from_words(\"的\", \"相对而言\", \"吧\")\n",
    "    print(\"不分词去掉停用词\", replace_stropwords_text(text, \"**\", trie))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "314.297px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
